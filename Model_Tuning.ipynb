{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, GRU, Embedding, Dropout, Activation, LSTM\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "# import gensim.models.keyedvectors as word2vec\n",
    "# import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'\n",
    "EMBEDDING_FILE=f'{path}glove.6B.50d.txt'\n",
    "TRAIN_DATA_FILE=f'{path}train.csv'\n",
    "TEST_DATA_FILE=f'{path}test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (159571, 8)\n",
      "Test shape: (153164, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "print(f'Train shape: {train.shape}')\n",
    "print(f'Test shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79786, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample = train.sample(frac=.5, random_state=29)\n",
    "\n",
    "train_smaple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train_sample[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_sample[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = [50, 100, 150, 200, 250, 300]\n",
    "max_features = [10000, 20000, 30000, 40000, 50000, 60000]\n",
    "max_len = [50, 100, 150, 200, 250, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(list_sentences_train, y, test_size=0.5, random_state=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020940498, 0.6441043)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First run for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Embed Size: 50, Max Features: 10000, Max Len: 50\n",
      "\n",
      "Epoch 1/2\n",
      "39893/39893 [==============================] - 213s - loss: 0.0784 - acc: 0.9749   \n",
      "Epoch 2/2\n",
      "39893/39893 [==============================] - 215s - loss: 0.0538 - acc: 0.9812   \n",
      "\n",
      "\n",
      "Predicting Embed Size: 50, Max Features: 10000, Max Len: 50\n",
      "\n",
      "39893/39893 [==============================] - 32s    \n",
      "\n",
      "\n",
      "ROC AUC for Embed Size: 50, Max Features: 10000, Max Len: 50\n",
      "\n",
      "Toxic: 0.9659148345736702\n",
      "S Tox: 0.9858846465464857\n",
      "Obs:   0.9803626068751001\n",
      "Thr:   0.9437008572793971\n",
      "Ins:   0.9744479072547431\n",
      "IDH:   0.9675825106615659\n",
      "\n",
      "\n",
      "Fitting Embed Size: 50, Max Features: 10000, Max Len: 100\n",
      "\n",
      "Epoch 1/2\n",
      "39893/39893 [==============================] - 407s - loss: 0.0771 - acc: 0.9750   \n",
      "Epoch 2/2\n",
      "39893/39893 [==============================] - 406s - loss: 0.0514 - acc: 0.9817   \n",
      "\n",
      "\n",
      "Predicting Embed Size: 50, Max Features: 10000, Max Len: 100\n",
      "\n",
      "39893/39893 [==============================] - 73s    \n",
      "\n",
      "\n",
      "ROC AUC for Embed Size: 50, Max Features: 10000, Max Len: 100\n",
      "\n",
      "Toxic: 0.9720859629115964\n",
      "S Tox: 0.9882220002255114\n",
      "Obs:   0.9857514429874108\n",
      "Thr:   0.9577805583060008\n",
      "Ins:   0.9786955043571468\n",
      "IDH:   0.9651374065628502\n",
      "\n",
      "\n",
      "Fitting Embed Size: 50, Max Features: 10000, Max Len: 150\n",
      "\n",
      "Epoch 1/2\n",
      "39893/39893 [==============================] - 571s - loss: 0.0825 - acc: 0.9730   \n",
      "Epoch 2/2\n",
      "39893/39893 [==============================] - 648s - loss: 0.0515 - acc: 0.9815   \n",
      "\n",
      "\n",
      "Predicting Embed Size: 50, Max Features: 10000, Max Len: 150\n",
      "\n",
      "39893/39893 [==============================] - 87s    \n",
      "\n",
      "\n",
      "ROC AUC for Embed Size: 50, Max Features: 10000, Max Len: 150\n",
      "\n",
      "Toxic: 0.9735823068060678\n",
      "S Tox: 0.9893010879142689\n",
      "Obs:   0.9860494967532247\n",
      "Thr:   0.9586632793214652\n",
      "Ins:   0.9793947985981021\n",
      "IDH:   0.9729438784406816\n",
      "\n",
      "\n",
      "Fitting Embed Size: 50, Max Features: 10000, Max Len: 200\n",
      "\n",
      "Epoch 1/2\n",
      "39893/39893 [==============================] - 696s - loss: 0.0791 - acc: 0.9750   \n",
      "Epoch 2/2\n",
      "39893/39893 [==============================] - 856s - loss: 0.0516 - acc: 0.9815   \n",
      "\n",
      "\n",
      "Predicting Embed Size: 50, Max Features: 10000, Max Len: 200\n",
      "\n",
      "39893/39893 [==============================] - 510s    \n",
      "\n",
      "\n",
      "ROC AUC for Embed Size: 50, Max Features: 10000, Max Len: 200\n",
      "\n",
      "Toxic: 0.9729364716928928\n",
      "S Tox: 0.988692698020244\n",
      "Obs:   0.9857432866582233\n",
      "Thr:   0.9528688647881126\n",
      "Ins:   0.9796613984313295\n",
      "IDH:   0.9712376697895575\n",
      "\n",
      "\n",
      "Fitting Embed Size: 50, Max Features: 10000, Max Len: 250\n",
      "\n",
      "Epoch 1/2\n",
      "39893/39893 [==============================] - 974s - loss: 0.0750 - acc: 0.9763   \n",
      "Epoch 2/2\n",
      "19808/39893 [=============>................] - ETA: 472s - loss: 0.0519 - acc: 0.9816"
     ]
    }
   ],
   "source": [
    "# Sould i also tune drop out?\n",
    "resul = pd.DataFrame(columns=['layer','embed_size','max_features','max_len','time_taken','toxic','severe_toxic','obscene','threat','insult','identity_have','average'])\n",
    "\n",
    "for es in embed_size:\n",
    "    for mf in max_features:\n",
    "        for ml in max_len:\n",
    "            start = time()\n",
    "            tokenizer = Tokenizer(num_words=mf)\n",
    "            tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "            list_tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
    "            list_tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
    "            X_t = pad_sequences(list_tokenized_train, maxlen=ml)\n",
    "            X_te = pad_sequences(list_tokenized_test, maxlen=ml)\n",
    "            \n",
    "            word_index = tokenizer.word_index\n",
    "            nb_words = min(mf, len(word_index))\n",
    "            embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, es))\n",
    "            for word, i in word_index.items():\n",
    "                if i >= mf: continue\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "            inp = Input(shape=(ml,))\n",
    "            x = Embedding(mf, es, weights=[embedding_matrix])(inp)\n",
    "            x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "            x = GlobalMaxPool1D()(x)\n",
    "            x = Dense(50, activation=\"relu\")(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "            x = Dense(6, activation=\"sigmoid\")(x)\n",
    "            model = Model(inputs=inp, outputs=x)\n",
    "            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            \n",
    "            print(f'Fitting Embed Size: {es}, Max Features: {mf}, Max Len: {ml}\\n')\n",
    "            model.fit(X_t, y_train, batch_size=32, epochs=2)\n",
    "            print('\\n')\n",
    "            print(f'Predicting Embed Size: {es}, Max Features: {mf}, Max Len: {ml}\\n')\n",
    "            y_pred = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "            print('\\n')\n",
    "            print(f'ROC AUC for Embed Size: {es}, Max Features: {mf}, Max Len: {ml}\\n')\n",
    "            \n",
    "            tox_rocauc = roc_auc_score(y_test[:,0], y_pred[:,0])\n",
    "            stox_rocauc = roc_auc_score(y_test[:,1], y_pred[:,1])\n",
    "            obs_rocauc = roc_auc_score(y_test[:,2], y_pred[:,2])\n",
    "            thr_rocauc = roc_auc_score(y_test[:,3], y_pred[:,3])\n",
    "            ins_rocauc = roc_auc_score(y_test[:,4], y_pred[:,4])\n",
    "            idh_rocauc = roc_auc_score(y_test[:,5], y_pred[:,5])\n",
    "            avg_rocauc = (tox_rocauc+stox_rocauc+obs_rocauc+thr_rocauc+ins_rocauc+idh_rocauc)/6\n",
    "\n",
    "            print(f'Toxic: {tox_rocauc}')\n",
    "            print(f'S Tox: {stox_rocauc}')\n",
    "            print(f'Obs:   {obs_rocauc}')\n",
    "            print(f'Thr:   {thr_rocauc}')\n",
    "            print(f'Ins:   {ins_rocauc}')\n",
    "            print(f'IDH:   {idh_rocauc}')\n",
    "            print(f'Avrg:  {avg_rocauc}')\n",
    "            print('\\n')\n",
    "            \n",
    "            end = time()\n",
    "            \n",
    "            resul.append(['LSTM', es, mf, ml, end - start, tox_rocauc, stox_rocauc, obs_rocauc, thr_rocauc, ins_rocauc, idh_rocauc, avg_rocauc])\n",
    "#             resul.write(f'\\n{es},{mf},{ml},{tox_rocauc},{stox_rocauc},{obs_rocauc},{thr_rocauc},{ins_rocauc},{idh_rocauc},{avg_rocauc}')\n",
    "            \n",
    "            \n",
    "\n",
    "resul.to_csv('./results.csv', float_format='%.3f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resul = pd.DataFrame(columns=['layer','embed_size','max_features','max_len','time_taken','toxic','severe_toxic','obscene','threat','insult','identity_have','average'])\n",
    "resul.append(['LSTM', 50, 10000, 50, 3, .9938, .9938, .9938, .9938, .9938, .9938, .9938])\n",
    "resul.to_csv('./results.csv', float_format='%.3f', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
