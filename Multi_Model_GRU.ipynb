{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/anaconda3/envs/ml/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, GRU, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'\n",
    "EMBEDDING_FILE=f'{path}glove.6B.50d.txt'\n",
    "TRAIN_DATA_FILE=f'{path}train.csv'\n",
    "TEST_DATA_FILE=f'{path}test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 50, 100, 200 and 300 \n",
    "embed_size = 50 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "# try 50, 100, 200 and 300 \n",
    "maxlen = 200 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020940498, 0.6441043)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load embedding matrix from [kernal](https://www.kaggle.com/sbongo/do-pretrained-embeddings-give-you-the-extra-edge/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbeddingMatrix(typeToLoad):\n",
    "        #load different embedding file from Kaggle depending on which embedding \n",
    "        #matrix we are going to experiment with\n",
    "        if(typeToLoad==\"glove\"):\n",
    "            EMBEDDING_FILE=f'{path}glove.twitter.27B.25d.txt'\n",
    "            embed_size = 25\n",
    "        elif(typeToLoad==\"word2vec\"):\n",
    "            word2vecDict = word2vec.KeyedVectors.load_word2vec_format(f'{path}GoogleNews-vectors-negative300.bin', binary=True)\n",
    "            embed_size = 300\n",
    "        elif(typeToLoad==\"fasttext\"):\n",
    "            EMBEDDING_FILE=f'{path}wiki.simple.vec'\n",
    "            embed_size = 300\n",
    "\n",
    "        if(typeToLoad==\"glove\" or typeToLoad==\"fasttext\" ):\n",
    "            embeddings_index = dict()\n",
    "            #Transfer the embedding weights into a dictionary by iterating through every line of the file.\n",
    "            f = open(EMBEDDING_FILE)\n",
    "            for line in f:\n",
    "                #split up line into an indexed array\n",
    "                values = line.split()\n",
    "                #first index is word\n",
    "                word = values[0]\n",
    "                #store the rest of the values in the array as a new array\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs #50 dimensions\n",
    "            f.close()\n",
    "            print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "        else:\n",
    "            embeddings_index = dict()\n",
    "            for word in word2vecDict.wv.vocab:\n",
    "                embeddings_index[word] = word2vecDict.word_vec(word)\n",
    "            print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            \n",
    "        gc.collect()\n",
    "        #We get the mean and standard deviation of the embedding weights so that we could maintain the \n",
    "        #same statistics for the rest of our own random generated weights. \n",
    "        all_embs = np.stack(list(embeddings_index.values()))\n",
    "        emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "        \n",
    "        nb_words = len(tokenizer.word_index)\n",
    "        #We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
    "        #the size will be Number of Words in Vocab X Embedding Size\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "        gc.collect()\n",
    "\n",
    "        #With the newly created embedding matrix, we'll fill it up with the words that we have in both \n",
    "        #our own dictionary and loaded pretrained embedding. \n",
    "        embeddedCount = 0\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            i-=1\n",
    "            #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            #and store inside the embedding matrix that we will train later on.\n",
    "            if embedding_vector is not None: \n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                embeddedCount+=1\n",
    "        print('total embedded:',embeddedCount,'common words')\n",
    "        \n",
    "        del(embeddings_index)\n",
    "        gc.collect()\n",
    "        \n",
    "        #finally, return the embedding matrix\n",
    "        return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try word2vec for multi then run all for multi (4x6) and average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000000 word vectors.\n",
      "total embedded: 66078 common words\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = loadEmbeddingMatrix('word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For embedings trained from train data see this [kernal](https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting toxic\n",
      "Epoch 1/2\n",
      "159571/159571 [==============================] - 447s 3ms/step - loss: 0.1168 - acc: 0.9571\n",
      "Epoch 2/2\n",
      "159571/159571 [==============================] - 444s 3ms/step - loss: 0.0873 - acc: 0.9667\n",
      "Predicting toxic\n",
      "153164/153164 [==============================] - 53s 347us/step\n",
      "\n",
      "\n",
      "Fitting severe_toxic\n",
      "Epoch 1/2\n",
      "159571/159571 [==============================] - 448s 3ms/step - loss: 0.0270 - acc: 0.9903\n",
      "Epoch 2/2\n",
      "159571/159571 [==============================] - 446s 3ms/step - loss: 0.0213 - acc: 0.9907\n",
      "Predicting severe_toxic\n",
      "153164/153164 [==============================] - 53s 347us/step\n",
      "\n",
      "\n",
      "Fitting obscene\n",
      "Epoch 1/2\n",
      "159571/159571 [==============================] - 452s 3ms/step - loss: 0.0643 - acc: 0.9773\n",
      "Epoch 2/2\n",
      "159571/159571 [==============================] - 452s 3ms/step - loss: 0.0461 - acc: 0.9826\n",
      "Predicting obscene\n",
      "153164/153164 [==============================] - 53s 348us/step\n",
      "\n",
      "\n",
      "Fitting threat\n",
      "Epoch 1/2\n",
      "159571/159571 [==============================] - 454s 3ms/step - loss: 0.0121 - acc: 0.9970\n",
      "Epoch 2/2\n",
      "159571/159571 [==============================] - 453s 3ms/step - loss: 0.0068 - acc: 0.9975\n",
      "Predicting threat\n",
      "153164/153164 [==============================] - 53s 347us/step\n",
      "\n",
      "\n",
      "Fitting insult\n",
      "Epoch 1/2\n",
      "159571/159571 [==============================] - 450s 3ms/step - loss: 0.0766 - acc: 0.9706\n",
      "Epoch 2/2\n",
      "159571/159571 [==============================] - 448s 3ms/step - loss: 0.0584 - acc: 0.9763\n",
      "Predicting insult\n",
      "153164/153164 [==============================] - 53s 346us/step\n",
      "\n",
      "\n",
      "Fitting identity_hate\n",
      "Epoch 1/2\n",
      "159571/159571 [==============================] - 454s 3ms/step - loss: 0.0278 - acc: 0.9917\n",
      "Epoch 2/2\n",
      "159571/159571 [==============================] - 451s 3ms/step - loss: 0.0187 - acc: 0.9934\n",
      "Predicting identity_hate\n",
      "153164/153164 [==============================] - 53s 346us/step\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resu = np.zeros(shape=(X_te.shape[0],6))\n",
    "\n",
    "for i in range(6):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "#     x = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Bidirectional(GRU(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print(f'Fitting {list_classes[i]}')\n",
    "    model.fit(X_t, y[:,i], batch_size=32, epochs=2)\n",
    "    \n",
    "    print(f'Predicting {list_classes[i]}')\n",
    "    resu[:,i] = model.predict([X_te], batch_size=1024, verbose=1)[:,0]\n",
    "    print('\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 162s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(153164, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resu[:,0] = preds[:,0]\n",
    "\n",
    "preds[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(f'{path}sample_submission.csv')\n",
    "sample_submission.shape\n",
    "sample_submission[list_classes] = resu\n",
    "sample_submission.to_csv('multi_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GRU for Stox, threat and IDH \n",
    "\n",
    "Use LSTM for tox, obs and ins\n",
    "\n",
    "tox is the lowest atm so find optimal params for lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n",
    "[x] Run the full notebook \n",
    "\n",
    "[ ] try different combos of max features and embed size with a subset of data to find a good combo \n",
    "\n",
    "[ ] find the best combos for toxic, obscene and insult\n",
    "\n",
    "[ ] try training a model for all using the multimodel preds in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
